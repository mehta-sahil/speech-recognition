{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGYgyoaXNVH9",
        "outputId": "39a5daa8-a769-40a9-864b-35b1345e96fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from jiwer import wer, cer\n",
        "import kagglehub\n",
        "\n",
        "def padcollate(batch):\n",
        "    spectrograms, transcripts, inputlengths, targetlengths = [], [], [], []\n",
        "    for waveform, transcript, inputlength, targetlength in batch:\n",
        "        spectrograms.append(waveform.squeeze(0).transpose(0, 1))\n",
        "        transcripts.append(transcript)\n",
        "        inputlengths.append(inputlength)\n",
        "        targetlengths.append(targetlength)\n",
        "    paddedspecs = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n",
        "    paddedtranscripts = nn.utils.rnn.pad_sequence(transcripts, batch_first=True)\n",
        "    return paddedspecs, paddedtranscripts, torch.tensor(inputlengths), torch.tensor(targetlengths)\n",
        "\n",
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, manifestfile, audiopath, charmap):\n",
        "        self.audiolist = pd.read_csv(manifestfile, sep='|', header=None, quoting=3)\n",
        "        self.audiopath = audiopath\n",
        "        self.charmap = charmap\n",
        "\n",
        "        melkwargs = {\n",
        "            'n_fft': 1024,\n",
        "            'win_length': 1024,\n",
        "            'hop_length': 256,\n",
        "            'n_mels': 80\n",
        "        }\n",
        "        self.featuremaker = torchaudio.transforms.MFCC(\n",
        "            sample_rate=22050,\n",
        "            n_mfcc=80,\n",
        "            melkwargs=melkwargs\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audiolist)\n",
        "    def __getitem__(self, index):\n",
        "        audioname = self.audiolist.iloc[index, 0]\n",
        "        transcripttext = self.audiolist.iloc[index, 2]\n",
        "        audiopath = os.path.join(self.audiopath, f\"{audioname}.wav\")\n",
        "        waveform, samplerate = torchaudio.load(audiopath)\n",
        "        features = self.featuremaker(waveform)\n",
        "        encodedtext = [self.charmap[char] for char in transcripttext.lower() if char in self.charmap]\n",
        "        inputlength = features.shape[1]\n",
        "        targetlength = len(encodedtext)\n",
        "        return features, torch.tensor(encodedtext), inputlength, targetlength\n",
        "\n",
        "class SpeechModel(nn.Module):\n",
        "    def __init__(self, numfeatures, numclasses):\n",
        "        super(SpeechModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=numfeatures, hidden_size=512, num_layers=3, bidirectional=True, batch_first=True)\n",
        "        self.classifier = nn.Linear(1024, numclasses)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def greedydecoder(output, indexmap, blanklabel='-'):\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodedtexts = []\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decoded = []\n",
        "        for j, index in enumerate(args):\n",
        "            if index != charmap[blanklabel]:\n",
        "                if j == 0 or index != args[j-1]:\n",
        "                    decoded.append(index.item())\n",
        "        decodedtexts.append(\"\".join([indexmap[c] for c in decoded if c in indexmap]))\n",
        "    return decodedtexts\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"--- Using device: {device} ---\")\n",
        "\n",
        "    print(\"Downloading the LJ Speech dataset...\")\n",
        "    path = kagglehub.dataset_download(\"mathurinache/the-lj-speech-dataset\")\n",
        "    print(\"Dataset downloaded to:\", path)\n",
        "\n",
        "    basepath = os.path.join(path, \"LJSpeech-1.1\")\n",
        "    metadatapath = os.path.join(basepath, \"metadata.csv\")\n",
        "    audiopath = os.path.join(basepath, \"wavs\")\n",
        "\n",
        "    characters = \"'-abcdefghijklmnopqrstuvwxyz \"\n",
        "    charmap = {char: i for i, char in enumerate(characters)}\n",
        "    indexmap = {i: char for i, char in enumerate(characters)}\n",
        "\n",
        "    dataset = SpeechDataset(metadatapath, audiopath, charmap)\n",
        "    traindata = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=padcollate, pin_memory=True, num_workers=2)\n",
        "\n",
        "    model = SpeechModel(numfeatures=80, numclasses=len(characters)).to(device)\n",
        "    lossfunction = nn.CTCLoss(blank=charmap['-'], zero_infinity=True).to(device) # Added zero_infinity\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    epochs = 1\n",
        "    print(\"\\n--- Starting model training (QUICK TEST)... ---\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for i, data in enumerate(traindata):\n",
        "            if i > 200:\n",
        "                print(\"...Stopping training early for this quick test...\")\n",
        "                break\n",
        "\n",
        "            spectrograms, transcripts, inputlengths, targetlengths = data\n",
        "            spectrograms, transcripts = spectrograms.to(device), transcripts.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(spectrograms)\n",
        "            output = nn.functional.log_softmax(output, dim=2).transpose(0, 1)\n",
        "            loss = lossfunction(output, transcripts, inputlengths, targetlengths)\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            if i % 50 == 0:\n",
        "                print(f\"  Epoch {epoch+1}, Batch {i}, Current Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(\"\\n--- Starting evaluation... ---\")\n",
        "    model.eval()\n",
        "    allpredictions, alltargets = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(traindata):\n",
        "            if i > 20:\n",
        "                break\n",
        "            spectrograms, transcripts, _, _ = data\n",
        "            spectrograms = spectrograms.to(device)\n",
        "            output = model(spectrograms)\n",
        "            predictions = greedydecoder(output.cpu(), indexmap)\n",
        "            allpredictions.extend(predictions)\n",
        "            targets = [\"\".join([indexmap[c.item()] for c in text]) for text in transcripts]\n",
        "            alltargets.extend(targets)\n",
        "\n",
        "    worderror = wer(alltargets, allpredictions)\n",
        "    charerror = cer(alltargets, allpredictions)\n",
        "    print(\"\\n--- Quick Test Evaluation Complete ---\")\n",
        "    print(f\"Sample Prediction: '{allpredictions[0]}'\")\n",
        "    print(f\"Sample Target:     '{alltargets[0]}'\")\n",
        "    print(f\"\\nWord Error Rate (WER): {worderror:.4f}\")\n",
        "    print(f\"Character Error Rate (CER): {charerror:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WofMRrLWM1Ck",
        "outputId": "1cbca4aa-d6b8-4756-c857-41b737b33a85"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Using device: cuda ---\n",
            "Downloading the LJ Speech dataset...\n",
            "Using Colab cache for faster access to the 'the-lj-speech-dataset' dataset.\n",
            "Dataset downloaded to: /kaggle/input/the-lj-speech-dataset\n",
            "\n",
            "--- Starting model training (QUICK TEST)... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1, Batch 0, Current Loss: 0.4750\n",
            "  Epoch 1, Batch 50, Current Loss: 1.1134\n",
            "  Epoch 1, Batch 100, Current Loss: 1.1374\n",
            "  Epoch 1, Batch 150, Current Loss: 1.3784\n",
            "  Epoch 1, Batch 200, Current Loss: 0.5944\n",
            "...Stopping training early for this quick test...\n",
            "\n",
            "--- Starting evaluation... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Quick Test Evaluation Complete ---\n",
            "Sample Prediction: ''\n",
            "Sample Target:     'it was particularly recommended by the committee on jails in eighteen fourteen''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
            "\n",
            "Word Error Rate (WER): 1.0000\n",
            "Character Error Rate (CER): 1.0000\n"
          ]
        }
      ]
    }
  ]
}